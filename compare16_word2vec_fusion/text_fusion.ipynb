{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logging.handlers\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import socket\n",
    "from distutils.dir_util import copy_tree\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import natsort\n",
    "import datetime\n",
    "import pickle\n",
    "from pdb import set_trace as bp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "label = {'Not_Depressed', 'Depressed'}\n",
    "LABELS = {label: i for i, label in enumerate(label)}\n",
    "num_of_classes = len(LABELS)\n",
    "f_score = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(target, predict, classes_num, f_score_average):\n",
    "    \"\"\"\n",
    "    Calculates accuracy, precision, recall, F1-Score, True Negative,\n",
    "    False Negative, True Positive, and False Positives of the output of\n",
    "    the model\n",
    "\n",
    "    Inputs\n",
    "      target: np.array() The labels for the predicted outputs from the model\n",
    "      predict: np.array() The batched outputs of the network\n",
    "      classes_num: int How many classes are in the dataset\n",
    "      f_score_average: str How to average the F1-Score\n",
    "\n",
    "    Outputs:\n",
    "      accuracy: Float Accuracy of the model outputs\n",
    "      p_r_f: Array of Floats Precision, Recall, and F1-Score\n",
    "      tn_fp_fn_tp: Array of Floats True Negative, False Positive,\n",
    "                   False Negative, and True Positive\n",
    "    \"\"\"\n",
    "\n",
    "    number_samples_labels = len(target)\n",
    "\n",
    "    number_correct_predictions = np.zeros(classes_num)\n",
    "    total = np.zeros(classes_num)\n",
    "\n",
    "    for n in range(number_samples_labels):\n",
    "        total[target[n]] += 1\n",
    "\n",
    "        if target[n] == predict[n]:\n",
    "            number_correct_predictions[target[n]] += 1\n",
    "\n",
    "    con_matrix = confusion_matrix(target,\n",
    "                                  predict)\n",
    "    tn_fp_fn_tp = con_matrix.ravel()\n",
    "    if tn_fp_fn_tp.shape != (4,):\n",
    "        value = int(tn_fp_fn_tp)\n",
    "        if target[0][0] == 1:\n",
    "            tn_fp_fn_tp = np.array([0, 0, 0, value])\n",
    "        elif target[0][0] == 0:\n",
    "            tn_fp_fn_tp = np.array([value, 0, 0, 0])\n",
    "        else:\n",
    "            print('Error in the true_neg/false_pos value')\n",
    "            sys.exit()\n",
    "\n",
    "    if f_score_average is None:\n",
    "        # This code fixes the divide by zero error\n",
    "        accuracy = np.divide(number_correct_predictions,\n",
    "                             total,\n",
    "                             out=np.zeros_like(number_correct_predictions),\n",
    "                             where=total != 0)\n",
    "        p_r_f = metrics.precision_recall_fscore_support(target,\n",
    "                                                        predict)\n",
    "    elif f_score_average == 'macro':\n",
    "        # This code fixes the divide by zero error\n",
    "        accuracy = np.divide(number_correct_predictions,\n",
    "                             total,\n",
    "                             out=np.zeros_like(number_correct_predictions),\n",
    "                             where=total != 0)\n",
    "        p_r_f = metrics.precision_recall_fscore_support(target,\n",
    "                                                        predict,\n",
    "                                                        average='macro')\n",
    "    elif f_score_average == 'micro':\n",
    "        # This code fixes the divide by zero error\n",
    "        accuracy = np.divide(np.sum(number_correct_predictions),\n",
    "                             np.sum(total),\n",
    "                             out=np.zeros_like(number_correct_predictions),\n",
    "                             where=total != 0)\n",
    "        p_r_f = metrics.precision_recall_fscore_support(target,\n",
    "                                                        predict,\n",
    "                                                        average='micro')\n",
    "    else:\n",
    "        raise Exception('Incorrect average!')\n",
    "\n",
    "    if p_r_f[0].shape == (1,):\n",
    "        temp = np.zeros((4, 2))\n",
    "        position = int(target[0])\n",
    "        for val in range(len(p_r_f)):\n",
    "            temp[val][position] = float(p_r_f[val])\n",
    "\n",
    "        p_r_f = (temp[0], temp[1], temp[2], temp[3])\n",
    "\n",
    "    return accuracy, p_r_f, tn_fp_fn_tp\n",
    "\n",
    "def prediction_and_accuracy(batch_output, batch_labels, initial_condition,\n",
    "                            num_of_classes, complete_results, loss,\n",
    "                            per_epoch_pred, f_score_average=None):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy (including F1-Score) of the predictions from a\n",
    "    model. Also the True Negatives, False Negatives, True Positives, and False\n",
    "    Positives are calculated. These results are stored along with results\n",
    "    from previous epochs.\n",
    "\n",
    "    Input\n",
    "        batch_output: The output from the model\n",
    "        batch_labels: The respective labels for the batched output\n",
    "        initial_condition: Bool - True if this is the first instance to set\n",
    "                           up the variables for logging accuracy\n",
    "        num_of_classes: The number of classes in this dataset\n",
    "        complete_results: np.array - holds results for each iteration of\n",
    "                                     experiment\n",
    "        loss: The value of the loss from the current epoch\n",
    "        per_epoch_pred: Combined batch outputs and labels for record keeping\n",
    "        f_score_average: The type of averaging to be used fro the F1-Score (\n",
    "                         Macro, Micro, or None\n",
    "\n",
    "    Output\n",
    "        complete_results: np.array - holds results for each iteration of\n",
    "                                     experiment\n",
    "        per_epoch_pred: Combined results of batch outputs and labels for\n",
    "                        current epoch\n",
    "    \"\"\"\n",
    "    if type(batch_output) is not np.ndarray:\n",
    "        batch_output = batch_output.data.cpu().numpy()\n",
    "        batch_labels = batch_labels.data.cpu().numpy()\n",
    "\n",
    "    if len(batch_output.shape) == 1:\n",
    "        batch_output = batch_output.reshape(-1, 1)\n",
    "    if len(batch_labels.shape) == 1:\n",
    "        batch_labels = batch_labels.reshape(-1, 1)\n",
    "    if initial_condition:\n",
    "        per_epoch_pred = np.hstack((batch_output, batch_labels))\n",
    "    else:\n",
    "        temp_stack = np.hstack((batch_output, batch_labels))\n",
    "        per_epoch_pred = np.vstack((per_epoch_pred, temp_stack))\n",
    "\n",
    "    prediction = np.round(batch_output)\n",
    "    prediction = prediction.reshape(-1)\n",
    "\n",
    "    if len(batch_labels.shape) > 1:\n",
    "        batch_labels = batch_labels.reshape(-1)\n",
    "    if batch_labels.dtype == 'float32':\n",
    "        batch_labels = batch_labels.astype(np.long)\n",
    "\n",
    "    acc, fscore, tn_fp_fn_tp = calculate_accuracy(batch_labels,\n",
    "                                                  prediction,\n",
    "                                                  num_of_classes,\n",
    "                                                  f_score_average)\n",
    "    complete_results[0:2] += acc\n",
    "    complete_results[2:8] += np.array(fscore[0:3]).reshape(1, -1)[0]\n",
    "    complete_results[10] += loss\n",
    "    complete_results[11:15] += tn_fp_fn_tp\n",
    "\n",
    "    return complete_results, per_epoch_pred\n",
    "\n",
    "def evaluation_for_test(results_dict, num_class, f_score,prediction_metric, hidden_test=False, verbose=False):\n",
    "    \"\"\"\n",
    "    This function is only used for mode==test and data_type=='test'. Every\n",
    "    prediction for each folder in the test set is accumulated to results_dict\n",
    "    along with the number of instances of each folder. There will be multiple\n",
    "    predictions for each folder depending on how many times the experiment\n",
    "    was run during training (e.g. 5). If the user sets argument:\n",
    "    prediction_metric=0 -> the accuracy will be determined for every experiment\n",
    "    iteration (e.g. 5) and the best performing model will be selected. NOTE:\n",
    "    This will not work if running in test mode without validation set and\n",
    "    using the test_split_Depression_AVEC2017.csv file.\n",
    "    prediction_metric=1 -> the average of the accumulated predictions for\n",
    "    each folder will be taken and the final score will relate to these\n",
    "    averaged results.\n",
    "    prediction_metric=2 -> The majority vote of the accumulated predictions\n",
    "    for each folder will be taken and the final score will related to these\n",
    "    results\n",
    "    Input\n",
    "        results_dict: dictionary key: output, target, accum. For each of\n",
    "                      these keys is a corresponding dictionary where key:\n",
    "                      folder, value relates to the super key: output ->\n",
    "                      predictions from experiments, target -> corresponding\n",
    "                      label for the folder, accum -> the accumulated\n",
    "                      instances of each folder\n",
    "        num_class: int - Number of classes in the dataset\n",
    "        f_score: str - Type of F1 Score processing\n",
    "    Outputs:\n",
    "        scores: List - contains accuracy, fscore and tn_fp_fn_tp\n",
    "    \"\"\"\n",
    "    if not hidden_test:\n",
    "        temp_tar = np.array(list(results_dict['target'].values()))\n",
    "    final_results = {}\n",
    "    exp_runthrough = len(results_dict['prob'][346])\n",
    "    # print(\"exp_runthrough: \", exp_runthrough)\n",
    "    # Pick best performing model\n",
    "    if prediction_metric == 0:\n",
    "        if verbose: print(\"\\nBest of predictions\")\n",
    "        f_score_avg = []\n",
    "        temp_out = np.zeros((exp_runthrough,\n",
    "                             len(results_dict['prob'].keys())))\n",
    "        temp_scores = np.zeros((exp_runthrough, 15))\n",
    "        for pos, f in enumerate(results_dict['prob'].keys()):\n",
    "            if exp_runthrough == 1:\n",
    "                temp_out[0, pos] = results_dict['prob'][f]\n",
    "            else:\n",
    "                temp_out[:, pos] = list(results_dict['prob'][f])\n",
    "        for exp in range(exp_runthrough):\n",
    "            temp_scores[exp, :], _ = prediction_and_accuracy(temp_out[exp, :],\n",
    "                                                             temp_tar,\n",
    "                                                             True,\n",
    "                                                             num_class,\n",
    "                                                             np.zeros(15),\n",
    "                                                             0,\n",
    "                                                             0,\n",
    "                                                             f_score)\n",
    "            f_score_avg = f_score_avg + [np.mean(temp_scores[exp, 6:8])]\n",
    "        best_result_index = f_score_avg.index(max(f_score_avg))\n",
    "        # print(f\"\\nThe best performing model was experiment: \"\n",
    "        #       f\"{best_result_index+1}\")\n",
    "        scores = temp_scores[best_result_index, :]\n",
    "    # Average the performance of all models\n",
    "    elif prediction_metric == 1:\n",
    "        if verbose: print(\"\\nAverage of predictions\")\n",
    "        for f in results_dict['prob'].keys():\n",
    "            final_results[f] = np.average(results_dict['prob'][f])\n",
    "        temp_out = np.array(list(final_results.values()))\n",
    "        if not hidden_test:\n",
    "            scores, _ = prediction_and_accuracy(temp_out,\n",
    "                                                temp_tar,\n",
    "                                                True,\n",
    "                                                num_class,\n",
    "                                                np.zeros(15),\n",
    "                                                0,\n",
    "                                                0,\n",
    "                                                f_score)\n",
    "        else:\n",
    "            scores = temp_out\n",
    "    # Calculate majority vote from all models\n",
    "    elif prediction_metric == 2:\n",
    "        if verbose: print(\"\\nMajority vote of predictions\")\n",
    "        for f in results_dict['prob'].keys():\n",
    "            final_results[f] = np.average(np.round(results_dict['prob'][f]))\n",
    "        temp_out = np.array(list(final_results.values()))\n",
    "        if not hidden_test:\n",
    "            scores, _ = prediction_and_accuracy(temp_out,\n",
    "                                                temp_tar,\n",
    "                                                True,\n",
    "                                                num_class,\n",
    "                                                np.zeros(15),\n",
    "                                                0,\n",
    "                                                0,\n",
    "                                                f_score)\n",
    "        else:\n",
    "            scores = temp_out\n",
    "    if not hidden_test:\n",
    "        scores[8] = np.mean(scores[0:2])\n",
    "        scores[9] = np.mean(scores[6:8])\n",
    "        scores = [scores[8], scores[0], scores[1], scores[9], scores[6], scores[7],\n",
    "                  scores[11], scores[12], scores[13], scores[14]]\n",
    "    else:\n",
    "        scores = np.round(scores)\n",
    "\n",
    "    return scores\n",
    "\n",
    "def print_results(results_dict1, results_dict2, merged_results_dict, verbose=False):\n",
    "    \n",
    "    # Best Model\n",
    "    speech_only = []\n",
    "    text_only = []\n",
    "    merged = []\n",
    "\n",
    "    for i in range(3):\n",
    "        comp_scores_1 = evaluation_for_test(results_dict1, num_of_classes, f_score,prediction_metric=i, hidden_test=False)\n",
    "        comp_scores_2 = evaluation_for_test(results_dict2, num_of_classes, f_score,prediction_metric=i, hidden_test=False)\n",
    "        comp_scores_merged = evaluation_for_test(merged_results_dict, num_of_classes, f_score,prediction_metric=i, hidden_test=False)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"#\"*50)\n",
    "            print(\"Speech-only\")\n",
    "            print('Acc(avg): {:.4f}, Acc(ND): {:.4f}, Acc(D): {:.4f}\\n F1(avg): {:.4f}, F1(ND): {:.4f}, F1(D): {:.4f}\\n tn: {}, fp: {}\\n fn: {}, tp: {}'.format(comp_scores_1[0],\\\n",
    "            comp_scores_1[1],comp_scores_1[2],comp_scores_1[3],comp_scores_1[4],comp_scores_1[5],comp_scores_1[6],comp_scores_1[7],comp_scores_1[8],comp_scores_1[9],))\n",
    "            \n",
    "            print(\"Text-only\")\n",
    "            print('Acc(avg): {:.4f}, Acc(ND): {:.4f}, Acc(D): {:.4f}\\n F1(avg): {:.4f}, F1(ND): {:.4f}, F1(D): {:.4f}\\n tn: {}, fp: {}\\n fn: {}, tp: {}'.format(comp_scores_2[0],\\\n",
    "            comp_scores_2[1],comp_scores_2[2],comp_scores_2[3],comp_scores_2[4],comp_scores_2[5],comp_scores_2[6],comp_scores_2[7],comp_scores_2[8],comp_scores_2[9],))\n",
    "\n",
    "            print(\"Speech+Text\")\n",
    "            print('Acc(avg): {:.4f}, Acc(ND): {:.4f}, Acc(D): {:.4f}\\n F1(avg): {:.4f}, F1(ND): {:.4f}, F1(D): {:.4f}\\n tn: {}, fp: {}\\n fn: {}, tp: {}'.format(comp_scores_merged[0],\\\n",
    "            comp_scores_merged[1],comp_scores_merged[2],comp_scores_merged[3],comp_scores_merged[4],comp_scores_merged[5],comp_scores_merged[6],comp_scores_merged[7],comp_scores_merged[8],comp_scores_merged[9],))\n",
    "        else:\n",
    "            speech_only.append(comp_scores_1[3])\n",
    "            text_only.append(comp_scores_2[3])\n",
    "            merged.append(comp_scores_merged[3])\n",
    "\n",
    "    if not verbose:\n",
    "        print('{:.4f},{:.4f},{:.4f},{:.4f},{:.4f},{:.4f},{:.4f},{:.4f},{:.4f}'.format(\\\n",
    "            speech_only[0],text_only[0], merged[0]\\\n",
    "            ,speech_only[1],text_only[1], merged[1]\\\n",
    "            ,speech_only[2],text_only[2], merged[2]))\n",
    "\n",
    "def get_results_prob(results_dict):\n",
    "    '''\n",
    "    This is a function to get the results of the experiments in terms of probabilities\n",
    "    '''\n",
    "    results_dict['prob'] = {}\n",
    "    for key in results_dict['output'].keys():\n",
    "        results_dict['prob'][key] = np.array(results_dict['output'][key]/results_dict['accum'][key])\n",
    "    \n",
    "    return results_dict\n",
    "\n",
    "def merge_results(results_dict1, results_dict2):\n",
    "    '''\n",
    "    This is a function to merge the results of two experiments into one\n",
    "    :param results_dict1: dictionary key: output, target, accum. For each of these keys is a corresponding dictionary where key: folder, value relates to the super key: output -> predictions from experiments, target -> corresponding label for the folder, accum -> the accumulated instances of each folder\n",
    "    :param results_dict2: dictionary key: output, target, accum. For each of these keys is a corresponding dictionary where key: folder, value relates to the super key: output -> predictions from experiments, target -> corresponding label for the folder, accum -> the accumulated instances of each folder\n",
    "\n",
    "    :return: results_dict: dictionary key: output, target, accum. For each of these keys is a corresponding dictionary where key: folder, value relates to the super key: output -> predictions from experiments, target -> corresponding label for the folder, accum -> the accumulated instances of each folder\n",
    "\n",
    "\n",
    "    '''\n",
    "    results_dict = {}\n",
    "    results_dict['prob'] = {}\n",
    "    results_dict['target'] = {}\n",
    "    for key in results_dict1['output'].keys():\n",
    "        results_dict['prob'][key] = np.concatenate([results_dict1['prob'][key], results_dict2['prob'][key]])\n",
    "        results_dict['target'][key] = results_dict1['target'][key] \n",
    "    return results_dict\n",
    "\n",
    "def get_results_dicts(exp_dir1, exp_dir2):\n",
    "\n",
    "    with open(exp_dir1+'/accum_results_dict_total.pickle','rb') as f:\n",
    "        results_dict1 = pickle.load(f)\n",
    "    results_dict1 = get_results_prob(results_dict1)\n",
    "    \n",
    "    with open(exp_dir2+'/accum_results_dict_total.pickle','rb') as f:\n",
    "        results_dict2 = pickle.load(f)\n",
    "    results_dict2 = get_results_prob(results_dict2)\n",
    "\n",
    "    merged_results_dict = merge_results(results_dict1, results_dict2)\n",
    "    \n",
    "    return results_dict1, results_dict2, merged_results_dict\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download folders and change path here. \n",
    "text_folder = 'text_cnn_lstm_feat_text_dim_9_batch_20_lr_0.0001_wd_0_lrf_2_alpha_0'\n",
    "audio_folder = 'cnn_lstm_feature_compare16_delta_feat_dim_384_batch_20_lr_0.003_wdecay_0_lrf_2_alpha_4e-06'\n",
    "results_dict1, results_dict2, merged_results_dict = get_results_dicts(audio_folder, text_folder)\n",
    "print_results(results_dict1, results_dict2, merged_results_dict,verbose)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 ('daic')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8bfc193a2751328e169a060aba9d3e5d48033e8a9c82259135301c386f340220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
